{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1300f18d-5468-4ae4-ad52-e6b50b53e92e",
   "metadata": {},
   "source": [
    "# Web Spider Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd20aa-4d4d-48c0-aa33-00b82cbf0d76",
   "metadata": {},
   "source": [
    "## 1. 网络爬虫简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414ec65-d962-4cf9-80de-5b5d80aa4b07",
   "metadata": {},
   "source": [
    "**网络爬虫**指通过**脚本**自动从网页获取**数据**（文本、图像、音频、视频），并保存至本地的过程。例如：\n",
    "+ 获取人民网近三个月的全部新闻\n",
    "+ 给定关键词，从百度图片获取100张和关键词有关的图片\n",
    "+ 获取给定BV号的B站视频"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023dc28b-6e1f-40ce-bc56-1610240ac9ee",
   "metadata": {},
   "source": [
    "## 2. 基于`requests`和`BeautifulSoup`的Python网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d915b15-f3a8-4656-905a-7e7fc90a0858",
   "metadata": {},
   "source": [
    "基于Python的网络爬虫一般可以通过两个模块实现：`requests`和`BeautifulSoup`。\n",
    "+ `requests`：用于向url**发出请求**，**获取响应**\n",
    "+ `BeautifulSoup`：用于**解析**通过`requests`获得的响应，**获取所需数据**。其有时会配合正则表达式模块`re`一起使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde1c18-598c-4a68-a4b5-7d887dd4bd7b",
   "metadata": {},
   "source": [
    "## 3. 本教程项目介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6211d-303a-4c94-8385-0d573b3f4332",
   "metadata": {},
   "source": [
    "本教程通过B站爬虫项目介绍爬虫的相关概念和基本流程。\n",
    "+ 输入：一个BV号，如`BV19B4y1W76i`\n",
    "+ 输出：视频基本信息（标题、时间、简介、标签、分p信息）和视频内容（mp4格式。如果视频有分p，获取每一p的内容）。\n",
    "\n",
    "接下来，我们以获取BV号为`BV19B4y1W76i`的视频的基本信息和内容为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8606588-db71-49ec-b4e0-5e26d26e31ff",
   "metadata": {},
   "source": [
    "### 3.1 B站视频BV号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71be60-a6a0-4701-933c-98bd80da1d34",
   "metadata": {},
   "source": [
    "首先，介绍B站视频BV号（即视频ID）的获取方法。\n",
    "\n",
    "1. 打开某个B站的视频。\n",
    "2. 获取其url。url可能是这样的：\n",
    "    + https://www.bilibili.com/video/BV19B4y1W76i?spm_id_from=333.337.search-card.all.click&vd_source=cb6bdc56db66ac895c0f3d6912c94028\n",
    "3. 红色字体部分即为BV号。<font color=gray><i><u>另外，BV号后面的参数信息（即?及其后面的内容）的有无不影响对该url的访问。</u></i></font>\n",
    "    + https:</div>//www</div>.bilibili.com/video/<font color=red>BV19B4y1W76i</font>?spm_id_from=333.337.search-card.all.click&vd_source=cb6bdc56db66ac895c0f3d6912c94028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b9348-8db8-49de-88ba-553992329cba",
   "metadata": {},
   "source": [
    "相反，给定视频BV号（如`BV19B4y1W76i`），我们也可以构造这个视频的url。在BV号前面加上`https://www.bilibili.com/video/`即可得到该BV号对应的url，即：\n",
    "+ https://www.bilibili.com/video/BV19B4y1W76i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d41e38-eb16-4c4c-9ebe-eaf448e228c0",
   "metadata": {},
   "source": [
    "### 3.2 通过BV号构建视频网址的url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be477e15-d8a2-4f48-b20a-22e01bbf7faa",
   "metadata": {},
   "source": [
    "基于上述介绍，我们可以写一个函数`construct_url`，接收BV号，返回视频的url。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3ab519-e2f9-4c0c-abe5-b253bb900c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_url(bv_id):\n",
    "    \"\"\"Construct a url with the given bv id.\n",
    "\n",
    "    :param bv_id: BV id.\n",
    "    :return: Constructed url.\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"https://www.bilibili.com/video/{bv_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949994fe-e5c6-4d1e-bde8-0c336757f731",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 函数构建\n",
    "+ 函数文档\n",
    "+ 字符串格式化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bcb84-1408-40e4-a78e-f77477386f9f",
   "metadata": {},
   "source": [
    "现在我们来测试一下这个函数。将BV号`BV19B4y1W76i`作为传入函数，获取构造的url。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98e87b3-88f1-4d19-9a0f-0f1eac4e461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bilibili.com/video/BV19B4y1W76i\n"
     ]
    }
   ],
   "source": [
    "bv_id = \"BV19B4y1W76i\"\n",
    "url = construct_url(bv_id=bv_id)\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8608a7-088f-4904-88b9-bc4a4cfea872",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 函数调用\n",
    "+ 输出语句"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28417b-b3a7-4e4b-9707-6f2512ce9888",
   "metadata": {},
   "source": [
    "## 4. 使用`requests`模块请求网页内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc931ab7-76bf-4d58-94a8-9c7850462f25",
   "metadata": {},
   "source": [
    "爬虫的第一步是**向网页的url发出请求，并获取响应（即网页内容，一般为html）**，我们使用`requests`模块完成这个任务。\n",
    "\n",
    "我们需要获取的内容可以通过浏览器直观看到，步骤如下：\n",
    "1. 通过url访问网页，如：https://www.bilibili.com/video/BV19B4y1W76i\n",
    "2. 打开网页源代码窗口：鼠标右键，点击“查看网页源代码”（取决于浏览器，Chrome的叫法是这个。有些浏览器是“审查元素”）。或者也可以通过`F12`打开。打开的页面如下。左侧红框部分就是我们需要通过`requests`获取的内容，即网页的html。（注意，需要在顶部导航栏选择“Elements”（不同浏览器可能叫法不同）才能看到。不过一般默认显示的就是“Elements”的内容）\n",
    "\n",
    "<div align=\"center\"><img src=\"f12.png\" width=\"600\" align=\"center\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834771e3-ce88-4594-96a1-9738ca402e1d",
   "metadata": {},
   "source": [
    "### 4.1 主流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255a158-2d84-472e-9d77-8448c47f24a0",
   "metadata": {},
   "source": [
    "这一节介绍使用`requests`模块获取请求网页内容的主流程。分为两步：\n",
    "1. 向url发出请求，获取响应内容\n",
    "2. 从响应内容获取html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87687551-ea39-42b1-96f1-d86f13cf2f6f",
   "metadata": {},
   "source": [
    "首先，我们导入`requests`模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfefb4c7-bef4-4800-9a11-5d42cbd78694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc0454-1c84-4cab-9baa-2d4669f3dd09",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 模块导入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38484477-69aa-4b5a-b99e-16831a4dbcc4",
   "metadata": {},
   "source": [
    "#### 4.1.1 向url发出请求，获取响应内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c676bc-374e-4c35-a1ca-aaf68d0e8512",
   "metadata": {},
   "source": [
    "导入模块后，使用`requests`模块的`get`方法向刚刚构建的`url`发出请求，得到响应内容`r`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6d38f0-8eeb-451b-a06f-0b0e3e835d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ad06c-fbcc-451c-ba04-49f87cbb62ed",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 类方法的调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1bff13-f7f7-4452-998e-93c95d32598e",
   "metadata": {},
   "source": [
    "我们可以通过输出`r`查看请求是否成功。如果请求成功，将输出`<Response [200]>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe12ee3-123b-488d-8efb-8780fffec9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1b949-a178-4c3e-9d3a-067a1e9c2b8d",
   "metadata": {},
   "source": [
    "#### 4.1.2 从响应内容获取html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449d5c3-7fa8-4837-b75d-59053d327cde",
   "metadata": {},
   "source": [
    "获取响应内容`r`之后，我们通过`r`的`text`属性获取网页的html。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e76d8210-85a9-4e06-85eb-3380016cf5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"zh-Hans\"><head itemprop=\"video\" itemscope itemtype=\"http://schema.org/VideoObject\"><meta name=\"format-detection\" content=\"telephone=no, email=no\"><meta http-equiv=\"Content-Type\" content=\"text/html\" charset=\"utf-8\"><meta name=\"spm_prefix\" content=\"333.788\"><meta name=\"referrer\" content=\"no-referrer-when-downgrade\"><meta name=\"applicable-device\" content=\"pc\"><meta http-equiv=\"Cache-Control\" content=\"no-transform\"><meta http-equiv=\"Cache-Control\" content=\"no-siteapp\"><link rel=\"stylesheet\" href=\"//s1.hdslb.com/bfs/static/jinkela/long/laputa-css/map.css\"><link rel=\"stylesheet\" href=\"//s1.hdslb.com/bfs/static/jinkela/long/laputa-css/light_u.css\"><link id=\"__css-map__\" rel=\"stylesheet\" href=\"//s1.hdslb.com/bfs/static/jinkela/long/laputa-css/light.css\"><script type=\"text/javascript\">window.webAbTest={\"pc_player_autoplay_switch_reset\":\"1\",\"buvidsplit\":\"7\"}</script> <title data-vue-meta=\"true\">[中英字幕]吴恩达2022机器学习 machine learning specialization_哔哩哔哩_bilibili</title> <me\n"
     ]
    }
   ],
   "source": [
    "html = r.text\n",
    "\n",
    "# 输出html的前1000个字符\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cefeee-ad69-4415-a72e-edcabf0abb83",
   "metadata": {},
   "source": [
    "#### 4.1.3 主流程核心代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f25be8-e459-47ca-9da7-689dd68a38bf",
   "metadata": {},
   "source": [
    "基于此，主流程的核心代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51f73b9-f895-4131-aa74-beecccdd248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(url=url)\n",
    "html = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5d957-0b1a-47f3-86d2-a5fd7f827db4",
   "metadata": {},
   "source": [
    "### 4.3 处理网络请求的异常"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4bef9-54f0-40ef-aeab-ae0687beadaf",
   "metadata": {},
   "source": [
    "网络请求可能由于网络条件和服务器响应意愿等原因产生各种各样的异常，如请求超时、拒绝访问、不可达等问题。为了处理这些异常，通常将代码写成如下形式。主要有两个改动：\n",
    "1. 调用`get`方法后，通过`r.raise_for_status()`确认请求是否成功。如果请求不成功，这行代码会抛出异常。\n",
    "2. 通过异常处理`try-except`处理`raise_for_status`方法可能抛出的异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e065b1d-0c95-4ebd-83b1-08f88b3a2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\n",
    "try:\n",
    "    # Send the request.\n",
    "    r = requests.get(url=url)\n",
    "    # Raise an exception if something goes wrong.\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    html = r.text\n",
    "except:\n",
    "    html = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab8080-aaa3-4b5c-ab05-8f7bcf845707",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 异常处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921b9c3-972d-4bdb-b1a1-a82104f571c9",
   "metadata": {},
   "source": [
    "### 4.4 封装成函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb167c16-893e-4961-997c-8015ae3b3329",
   "metadata": {},
   "source": [
    "为了更好地模块化，现在我们将上面的请求网页内容的代码封装成函数`request`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7898d2a6-6912-4181-a54f-e99b146bc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(url):\n",
    "    \"\"\"Send a request.\n",
    "\n",
    "    :param url: The url to which the request is sent.\n",
    "    :return: Response from the server of the url.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Send the request.\n",
    "        r = requests.get(url=url)\n",
    "        # Raise an exception if something goes wrong.\n",
    "        r.raise_for_status()\n",
    "\n",
    "        return r\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba550e7-dba2-4444-a568-d8c0b56fd7aa",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 空值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96c1f2-5bb7-4a21-8492-694798ef3f49",
   "metadata": {},
   "source": [
    "需要注意，该函数返回的是请求成功后的响应对象`r`。不直接返回网页的html（即`r.text`）的原因是我们有时候请求的是音频和视频等二进制资源，而不是html。在请求二进制资源的时候，我们获取资源使用的是`r.content`。\n",
    "\n",
    "因此，我们通过以下代码获取请求的html。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110ea964-d6c8-49fa-bc71-cbe5285633e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = request(url=url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c84ba-6cd2-4264-81b7-25b0311e9965",
   "metadata": {},
   "source": [
    "## 5. 使用`BeautifulSoup`模块解析网页并获取所需内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ddf5a-1641-4e99-b062-d881177a9446",
   "metadata": {},
   "source": [
    "使用`requests`模块请求得到网页内容后，我们使用`BeautifulSoup`模块获取所需的内容，如文本、图片url、视频url等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7ed9e-dad9-40f7-aa99-a4fafbe287b2",
   "metadata": {},
   "source": [
    "### 5.1 主流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec9d54-d349-4b70-bfe9-cffc7e927492",
   "metadata": {},
   "source": [
    "这一节介绍使用`BeautifulSoup`解析并获取数据的主流程。包括：\n",
    "1. 解析html\n",
    "2. 通过浏览器定位所需数据对应的标签\n",
    "3. 在soup中获取标签对象\n",
    "4. 从标签对象中获取所需数据\n",
    "\n",
    "我们以获取视频的标题为例。上述BV号`BV19B4y1W76i`对应的视频标题为：\n",
    "+ [中英字幕]吴恩达2022机器学习 machine learning specialization。\n",
    "\n",
    "我们接下来从获得的`html`中获取该标题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac45f58-b50f-4a19-96dd-5034159e8783",
   "metadata": {},
   "source": [
    "#### 5.1.1 解析html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7a6d6-a681-4b35-a21d-0b0318d2e6d4",
   "metadata": {},
   "source": [
    "首先，导入该模块，并使用该模块解析通过`requests`得到的html。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be165940-117c-45a6-a37d-ef857e0ddf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the html.\n",
    "soup = BeautifulSoup(\n",
    "    markup=html,\n",
    "    features=\"html.parser\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4780e-ceba-4614-b906-aa99e0a085e9",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 模块导入（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce57d2-1cbd-4b59-bff7-da14d3fb050a",
   "metadata": {},
   "source": [
    "#### 5.1.2 通过浏览器定位所需数据对应的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16843c18-0c6d-4163-9f73-2912bb754be7",
   "metadata": {},
   "source": [
    "接下来，我们需要通过浏览器定位标题所在的标签：\n",
    "1. 打开网页源代码窗口（右键后点击“查看网页源代码”/“审查元素”，或者`F12`。）\n",
    "2. 点击左上角按钮\n",
    "3. 点击按钮后，在网页中点击视频标题，可以看到源代码窗口会定位到标题所在的标签位置。\n",
    "<div align=\"center\"><img src=\"locate_title.png\" width=\"800\" align=\"center\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97408bf9-5b3f-49de-85f1-df9b3a55a961",
   "metadata": {},
   "source": [
    "可以看到，定位到的标签是：\n",
    "```html\n",
    "<h1 title=\"[中英字幕]吴恩达2022机器学习 machine learning specialization\" class=\"video-title tit\">[中英字幕]吴恩达2022机器学习 machine learning specialization</h1>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4a139-1efd-403d-baf1-4ae7e8a41411",
   "metadata": {},
   "source": [
    "#### 5.1.3 在soup中获取标签对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2d1c9-ce4f-4abc-a7e6-b9c569a8e1e8",
   "metadata": {},
   "source": [
    "接下来，我们通过`find`方法从`soup`中获取这个标签对应的标签对象。`find`方法的参数`name`需要传入标签的类型。上述标签的类型是`h1`，因此传入`\"h1\"`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25e3583-0d5b-4d4e-8002-86fffc0cead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取到的标签对象：<h1 class=\"video-title tit\" title=\"[中英字幕]吴恩达2022机器学习 machine learning specialization\">[中英字幕]吴恩达2022机器学习 machine learning specialization</h1>\n",
      "该标签对象的类型是：<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "title_h1 = soup.find(name=\"h1\")\n",
    "\n",
    "# 看看获取到的标签对象。\n",
    "print(f\"获取到的标签对象：{title_h1}\")\n",
    "# 查看其类型。\n",
    "print(f\"该标签对象的类型是：{type(title_h1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a1341-4030-4c78-808f-387b59090eb8",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 查看变量类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2bca6-1dc9-427b-895c-2190d2a7096a",
   "metadata": {},
   "source": [
    "#### 5.1.4 从标签对象中获取所需数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c75e20-59e8-4bbb-bae5-77a96bea6490",
   "metadata": {},
   "source": [
    "接下来，我们从`title_h1`中获取标题。从上述标签可以看到，标题被两个`h1`标签夹起来，形式为`<h1>标题</h1>`。我们通过`text`属性即可访问这些被标签夹起来的内容，即此处的标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fd2408-e105-4351-84b7-11c936b41f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[中英字幕]吴恩达2022机器学习 machine learning specialization\n"
     ]
    }
   ],
   "source": [
    "title = title_h1.text\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e8a8e-e560-4900-9276-c41b90d41d8e",
   "metadata": {},
   "source": [
    "#### 5.1.5 主流程核心代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1a62f-0b9f-492c-abe8-587d2b325957",
   "metadata": {},
   "source": [
    "基于此，主流程的核心代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9f77bd-d4db-4a20-92cc-b3fb5bbf37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the html.\n",
    "soup = BeautifulSoup(\n",
    "    markup=html,\n",
    "    features=\"html.parser\"\n",
    ")\n",
    "\n",
    "# Get title.\n",
    "title_h1 = soup.find(name=\"h1\")  # 通过“5.1.2 通过浏览器定位所需数据对应的标签”得知这个标签是一个h1标签。\n",
    "title = title_h1.text  # 通过“5.1.2 通过浏览器定位所需数据对应的标签”得知标题被两个标签（一开一闭）夹起来，因此使用.text。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189cc31-72d3-4528-b0f0-81432543c087",
   "metadata": {},
   "source": [
    "### 5.2 一些类型的标签可能有很多个！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592a026-f3c6-4dd4-ac35-5014833a0c28",
   "metadata": {},
   "source": [
    "现在，我们尝试获取视频的时间，步骤类似。通过浏览器，我们可以看到视频时间的标签是一个`span`标签。（<font color=\"red\">定位后需要点击▶展开`span`标签才能看到日期</font>）\n",
    "\n",
    "<div align=\"center\"><img src=\"locate_date.png\" width=\"800\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa52527-42bf-4cd8-919f-0530b60aa15d",
   "metadata": {},
   "source": [
    "我们尝试获取日期的`span`标签对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b467f282-5d03-42c1-a8cf-fd85e6065638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"van-dialog__title\">确认公开发布笔记？</span>\n"
     ]
    }
   ],
   "source": [
    "date_span = soup.find(name=\"span\")\n",
    "\n",
    "# 输出看看\n",
    "print(date_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d57d3a-e4c3-4eaf-b62f-884fc4dec7e8",
   "metadata": {},
   "source": [
    "Ohhhhhhhhhhh。好像不是我们想要的。原因是，有很多很多个`span`标签（在网页源码`Ctrl F`搜索一下`<span>`就知道了）。`find`方法不知道你想要哪一个。\n",
    "\n",
    "<div align=\"center\"><img src=\"many_spans.png\" width=\"800\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffb727-d918-4c61-8577-59edbe503422",
   "metadata": {},
   "source": [
    "因此，我们需要更精确的查找。我们再看看包着视频日期的`span`标签：\n",
    "\n",
    "```html\n",
    "<span class=\"pudate item\">\n",
    "    <svg t=\"1642588113899\" viewBox=\"0 0 1024 1024\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" p-id=\"6827\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"200\" height=\"200\" class=\"icon\">\n",
    "        <defs></defs>\n",
    "        <path d=\"M167.024 512a344.976 344.976 0 1 1 689.952 0 344.976 344.976 0 0 1-690 0zM512 106.976a405.024 405.024 0 1 0 0 810.048 405.024 405.024 0 0 0 0-810z m30 235.008a30 30 0 1 0-60 0V512c0 7.968 3.168 15.6 8.784 21.216l120 120a30 30 0 1 0 42.432-42.432L542 499.52V341.984z\" fill=\"#9499A0\" p-id=\"6828\"></path>\n",
    "    </svg>\n",
    "\n",
    "      2022-06-16 09:38:53\n",
    "</span>\n",
    "```\n",
    "\n",
    "这个`span`标签有一个`class`属性，这个属性有两个值：`pudate`和`item`。通过搜索发现，`class`属性的这两个值中`pudate`只出现了一次，因此我们可以用它完成更精确的定位。我们只需在`find`方法中指定`class`属性对应的值即可。需要注意，<font color=\"red\"><b>由于标签的这个属性`class`和python的类定义关键字`class`冲突了，因此我们实际要写的是`class_`。</b></font>\n",
    "\n",
    "由于视频日期也被两个`span`标签夹着，即`<span>视频日期</span>`的形式，因此，也使用`text`获取。你可能会问，`span`标签里面还有一个`svg`标签，不会有影响吗？？其实是不会的。实际上，<font color=\"red\"><b>在很多时候，我们获取标签对象之后都可以先尝试使用`text`获取内容，一般不会有问题。有问题再想办法~</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d765e38-8114-49b7-bf6b-8149a413d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      2022-06-16 09:38:53\n"
     ]
    }
   ],
   "source": [
    "date_span = soup.find(\n",
    "    name=\"span\",\n",
    "    class_=\"pudate\"\n",
    ")\n",
    "date = date_span.text\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a577804-fa2a-4511-a59e-dc34d3dd83c3",
   "metadata": {},
   "source": [
    "很明显，我们还需要使用字符串方法`strip`去掉多余的空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3814f44c-afbc-4802-9174-f6e0420accf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 09:38:53\n"
     ]
    }
   ],
   "source": [
    "date = date.strip()\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f207fc-c261-4343-b000-829a98a9700f",
   "metadata": {},
   "source": [
    "<!-- ### 5.3 标签套娃以及`find_all` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318bead-5c8c-41e1-bbe4-28a342008656",
   "metadata": {},
   "source": [
    "<!-- 有时候，我们使用`find`获取标签对象后，还希望从这个标签对象中获取其包含的某些标签对象，此时，对获取到的标签对象继续`find`即可。\n",
    "\n",
    "还有时候，我们希望获取的是一系列的标签对象，如一个列表的每一行，这时候，需要使用`find_all`。\n",
    "\n",
    "下面，通过获取视频的分p信息介绍这两种情况的应对方法。 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383e9b9-a44b-48ea-a599-ce4c0a6461dc",
   "metadata": {},
   "source": [
    "## 6. `find_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9c7f9-9792-4d96-9bc8-2bd63e2cf266",
   "metadata": {},
   "source": [
    "有时候，我们希望获取一系列数据，如表格每一行。这时候，我们需要使用`find_all`。假如我们需要获取视频的全部tag，如下图。可以看到，每个tag都被一个`li`标签包着，而这一系列`li`标签被一个`ul`标签包着。\n",
    "\n",
    "<div align=\"center\"><img src=\"locate_tags.png\" width=\"800\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c2ac3-ae41-48fc-b190-cda6aa079d4e",
   "metadata": {},
   "source": [
    "首先，我们定位这个`ul`标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb1ba10e-fb6b-4e65-8133-df574fd1985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ul = soup.find(\n",
    "    name=\"ul\",\n",
    "    class_=\"tag-area\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae10405-9f02-4613-90ae-dec0a5d60f7b",
   "metadata": {},
   "source": [
    "接下来，我们需要获取这个`ul`里面的所有`li`，分别从这些`li`里面获取tag。为此，我们需要使用`find_all`。其使用方法和`find`类似。<font color=gray><i><u>（实际上调用`find`之后内部也会调用`find_all`，所以`find`实际上是`find_all`只找一个标签的特殊形式。）</u></i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6060c636-b081-41f8-b139-3b647096c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['编程', '科学', '知识', '科学科普', 'AI', '机器学习', '吴恩达', '公开课创作激励x新星计划', '']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag_li in tags_ul.find_all(name=\"li\"):\n",
    "    tag = tag_li.text.strip()\n",
    "    tags.append(tag)\n",
    "        \n",
    "# 输出看看。\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aafb4c-ba92-4762-a835-f1213ade9ab1",
   "metadata": {},
   "source": [
    "Python基础回顾：\n",
    "+ 列表\n",
    "+ for循环\n",
    "+ while循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ccf9f-a5aa-43a4-92eb-1032cbe7b76a",
   "metadata": {},
   "source": [
    "基本没问题，不过`tags`列表最后有一个空串。这是因为，实际上找到的有些`li`标签不是tag，`strip`之后会变成空串（长度为0）。因此，我们需要做个过滤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9ce9a0a-2d0c-453c-9a43-5fa0da40499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['编程', '科学', '知识', '科学科普', 'AI', '机器学习', '吴恩达', '公开课创作激励x新星计划']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for tag_li in tags_ul.find_all(name=\"li\"):\n",
    "    tag = tag_li.text.strip()\n",
    "    \n",
    "    # 实际上找到的有些li标签不是tag，strip之后会变成空串（长度为0）。因此做个过滤。\n",
    "    if len(tag) != 0:\n",
    "        tags.append(tag)\n",
    "        \n",
    "# 输出看看。\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b6ad2-f060-4af1-ba20-d24338046189",
   "metadata": {},
   "source": [
    "Pytohn基础回顾：\n",
    "+ if条件语句\n",
    "+ 字符串长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d6ef0-545d-49c6-8323-4c71dd44508b",
   "metadata": {},
   "source": [
    "## 7. 网页内容的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d739d-6bb6-4eb2-b904-df4317edb50a",
   "metadata": {},
   "source": [
    "有时候，通过浏览器看到的网页源码和使用`requests`得到的会有差异。\n",
    "\n",
    "举个例子，我们使用前面介绍的方法获取视频分p信息。\n",
    "<div align=\"center\"><img src=\"locate_pages.png\" width=\"800\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db73306-4866-4090-9efc-878e0bfca6f7",
   "metadata": {},
   "source": [
    "直接上代码~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dc38e87-a134-4b42-bfb3-f4a0eb8c58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n",
      "<li><!-- --></li>\n"
     ]
    }
   ],
   "source": [
    "print_count = 10  # 这是为了不要输出太多。\n",
    "\n",
    "for li_p in soup.find(name=\"ul\", class_=\"list-box\").find_all(\"li\"):\n",
    "    print(li_p)\n",
    "    \n",
    "    print_count -= 1\n",
    "    if print_count == 0:\n",
    "        break  # 输出10个看看就行了。剩下的无视掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ae27d-ded7-4286-8007-3342def4788a",
   "metadata": {},
   "source": [
    "可以看到，啥也没有。这是因为，这个`ul`里面的东西是浏览器渲染出来的。我们代码中通过`requests`得到的`html`解析出来的`soup`没有经过浏览器渲染，因此拿不到这里的信息。\n",
    "\n",
    "然而实际上，我们在`requests`得到的`html`里面直接搜索某一个p的标题，是能搜到的（直接在4.1.2节获取的`html`搜索就可以）。通过分析`requests`请求得到的`html`，我们发现分p信息藏在一个`script`标签里面（如下，仅展示一部分）。而这一部分信息通过浏览器是看不到的。因此，有时候通过浏览器看到的网页源码和使用`requests`得到的会有差异。如果我们通过浏览器定位得不到需要的东西，可以试试**直接在`requests`请求得到的`html`中通过搜索手动定位**。在这里，通过`html`中直接定位找到了所需的`script`标签。\n",
    "\n",
    "```html\n",
    "<script>window.__INITIAL_STATE__={\"aid\":600070218,\"bvid\":\"BV19B4y1W76i\",\"p\":1,\"episode\":\"\",\"videoData\":{\"bvid\":\"BV19B4y1W76i\",\"aid\":600070218,\"videos\":100,\"tid\":201,\"tname\":\"科学科普\",\"copyright\":2,\"pic\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Farchive\\u002F73ce549a9b791241679533da1af72dad1ec8ed90.jpg\",\"title\":\"[中英字幕]吴恩达2022机器学习 machine learning specialization\",\"pubdate\":1655343533,\"ctime\":1655342382,\"desc\":\"吴恩达2022新版机器学习 machine learning specialization\\r\\n欢迎进群交流，所有资源也会放到群内：484266833\\r\\n课程官网：https:\\u002F\\u002Fwww.coursera.org\\u002Fspecializations\\u002Fmachine-learning-introduction\\r\\nGitHub：https:\\u002F\\u002Fgithub.com\\u002Fkaieye\\u002F2022-Machine-Learning-Specialization\\r\\n课程代码及测验内容已更新完毕\\r\\n欢迎pull request\",\"desc_v2\":[{\"raw_text\":\"吴恩达2022新版机器学习 machine learning specialization\\r\\n欢迎进群交流，所有资源也会放到群内：484266833\\r\\n课程官网：https:\\u002F\\u002Fwww.coursera.org\\u002Fspecializations\\u002Fmachine-learning-introduction\\r\\nGitHub：https:\\u002F\\u002Fgithub.com\\u002Fkaieye\\u002F2022-Machine-Learning-Specialization\\r\\n课程代码及测验内容已更新完毕\\r\\n欢迎pull request\",\"type\":1,\"biz_id\":0}],\"state\":0,\"duration\":48921,\"rights\":{\"bp\":0,\"elec\":0,\"download\":1,\"movie\":0,\"pay\":0,\"hd5\":0,\"no_reprint\":0,\"autoplay\":1,\"ugc_pay\":0,\"is_cooperation\":0,\"ugc_pay_preview\":0,\"no_background\":0,\"clean_mode\":0,\"is_stein_gate\":0,\"is_360\":0,\"no_share\":0,\"arc_pay\":0,\"free_watch\":0},\"owner\":{\"mid\":295896691,\"name\":\"渚隰\",\"face\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Fface\\u002F894091fa465069c4690ca0452ec6d5beedc2db19.jpg\"},\"stat\":{\"aid\":600070218,\"view\":59232,\"danmaku\":258,\"reply\":185,\"favorite\":6903,\"coin\":939,\"share\":400,\"now_rank\":0,\"his_rank\":0,\"like\":1761,\"dislike\":0,\"evaluation\":\"\",\"argue_msg\":\"\",\"viewseo\":59232},\"dynamic\":\"\",\"cid\":747974480,\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"premiere\":null,\"teenage_mode\":0,\"is_chargeable_season\":false,\"no_cache\":false,\"pages\":[{\"cid\":747974480,\"page\":1,\"from\":\"vupload\",\"part\":\"1.1 欢迎来到机器学习!\",\"duration\":165,\"vid\":\"\",\"weblink\":\"\",\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"first_frame\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Fstoryff\\u002Fn22061601rlyy7ii8t77b1zm614sjchw_firsti.jpg\"},{\"cid\":747961770,\"page\":2,\"from\":\"vupload\",\"part\":\"1.2 机器学习的应用\",\"duration\":269,\"vid\":\"\",\"weblink\":\"\",\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"first_frame\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Fstoryff\\u002Fn220616153e4i8j9uhy6lx2c55ps5xkh_firsti.jpg\"},{\"cid\":747961854,\"page\":3,\"from\":\"vupload\",\"part\":\"2.1 什么是机器学习\",\"duration\":336,\"vid\":\"\",\"weblink\":\"\",\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"first_frame\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Fstoryff\\u002Fn22061615ajo6u4qba5ah14jc7ky5n85_firsti.jpg\"},{\"cid\":747961924,\"page\":4,\"from\":\"vupload\",\"part\":\"2.2 监督学习 part 1\",\"duration\":417,\"vid\":\"\",\"weblink\":\"\",\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"first_frame\":\"http:\\u002F\\u002Fi2.hdslb.com\\u002Fbfs\\u002Fstoryff\\u002Fn220616081d3niogxxcusuq9eep0su5l_firsti.jpg\"},{\"cid\":747964817,\"page\":5,\"from\":\"vupload\",\"part\":\"2.3 监督学习 part 2\",\"duration\":437,\"vid\":\"\",\"weblink\":\"\",\"dimension\":{\"width\":1280,\"height\":720,\"rotate\":0},\"first_frame\":\"http:\\u002F\\u002Fi1.hdslb.com\\u002Fbfs\\u002Fstoryff\\u002Fn22061608i0zfo2mr4uq52dwqny1v1ox_firsti.jpg\"},\n",
    "\n",
    "...\n",
    "</script>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5311c7-74e2-4deb-8330-12ac836e721b",
   "metadata": {},
   "source": [
    "此后，获取分p信息就不难了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17e29651-efe4-4f82-8a2d-b63e8de215dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p1]: 1.1 欢迎来到机器学习!\n",
      "[p2]: 1.2 机器学习的应用\n",
      "[p3]: 2.1 什么是机器学习\n",
      "[p4]: 2.2 监督学习 part 1\n",
      "[p5]: 2.3 监督学习 part 2\n",
      "[p6]: 2.4 非监督学习 part 1\n",
      "[p7]: 2.5 非监督学习 part 2\n",
      "[p8]: 2.6 Jupyter Notebooks\n",
      "[p9]: 3.1 线性回归模型 part 1\n",
      "[p10]: 3.2 线性回归模型 part 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "window_initial_state_script = \"\"  # 存放上面的script。\n",
    "\n",
    "for script in soup.find_all(name=\"script\"):  # 有很多script标签。\n",
    "    try:\n",
    "        # 在这做做过滤。需要的script标签的内容包含\"window.__INITIAL_STATE__\"和\"pages\"。\n",
    "        # 这里用.string而不是.text，因为.text得不到。\n",
    "        # .string和.text是有区别的，不过不用想太多，.text不行就用.string。\n",
    "        if \"window.__INITIAL_STATE__\" in script.string and \"pages\" in script.string:  \n",
    "            window_initial_state_script = script.string\n",
    "            break  # 拿到了就跑。\n",
    "    except TypeError:  # 这里做异常处理是因为，对于find_all找出来的script，有些调用.string会出错。\n",
    "        pass\n",
    "    \n",
    "# 使用正则表达式抽取script里面包含的json字符串。\n",
    "pattern = re.compile(pattern=r\"(\\{.*\\});\")\n",
    "window_initial_state_json = pattern.search(string=window_initial_state_script).group(1)\n",
    "\n",
    "# json转dict。\n",
    "window_initial_state_dict = json.loads(s=window_initial_state_json)\n",
    "\n",
    "# 找出分p信息。\n",
    "pages = window_initial_state_dict[\"videoData\"][\"pages\"]\n",
    "pages = {\n",
    "    int(page_dict['page']): page_dict['part']\n",
    "    for page_dict in pages\n",
    "}\n",
    "\n",
    "# 输出前10个看看。\n",
    "print_count = 10\n",
    "for page_id, page_name in pages.items():\n",
    "    print(f\"[p{page_id}]: {page_name}\")\n",
    "    \n",
    "    print_count -= 1\n",
    "    if print_count == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c28d78-f4af-4110-91f3-82a905a06f94",
   "metadata": {},
   "source": [
    "Pytohn基础回顾：\n",
    "+ *正则表达式\n",
    "+ *json字符串转字典\n",
    "+ 字典\n",
    "+ 列表/元组/字典推导式\n",
    "+ 类型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb7476-5def-497b-8e9c-a208fd01482c",
   "metadata": {},
   "source": [
    "## 8. 总结：视频信息的获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3d1cdb8-4e8f-4c06-bb4e-8e394f69f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[title]: [中英字幕]吴恩达2022机器学习 machine learning specialization\n",
      "[date]: 2022-06-16 09:38:53\n",
      "[intro]: 吴恩达2022新版机器学习 machine learning specialization\n",
      "欢迎进群交流，所有资源也会放到群内：484266833\n",
      "课程官网：https://www.coursera.org/specializations/machine-learning-introduction\n",
      "GitHub：https://github.com/kaieye/2022-Machine-Learning-Specialization\n",
      "课程代码及测验内容已更新完毕\n",
      "欢迎pull request\n",
      "[tags]: ['编程', '科学', '知识', '科学科普', 'AI', '机器学习', '吴恩达', '公开课创作激励x新星计划']\n",
      "[pages]: {1: '1.1 欢迎来到机器学习!', 2: '1.2 机器学习的应用', 3: '2.1 什么是机器学习', 4: '2.2 监督学习 part 1', 5: '2.3 监督学习 part 2', 6: '2.4 非监督学习 part 1', 7: '2.5 非监督学习 part 2', 8: '2.6 Jupyter Notebooks', 9: '3.1 线性回归模型 part 1', 10: '3.2 线性回归模型 part 2', 11: '3.3 代价函数', 12: '3.4 代价函数的直观理解', 13: '3.5 可视化代价函数', 14: '3.6 可视化的例子', 15: '4.1 梯度下降', 16: '4.2 实现梯度下降', 17: '4.3 梯度下降的直观理解', 18: '4.4 学习率', 19: '4.5 线性回归中的梯度下降', 20: '4.6 运行梯度下降', 21: '5.1 多类特征', 22: '5.2 向量化 part 1', 23: '5.3 向量化 part 2', 24: '5.4 多元线性回归的梯度下降法', 25: '6.1 特征缩放 part 1', 26: '6.2 特征缩放 part 2', 27: '6.3 检查梯度下降是否收敛', 28: '6.4 学习率的选择', 29: '6.5 特征工程', 30: '6.6 多项式回归', 31: '7.1 Motivations', 32: '7.2 逻辑(logistic)回归', 33: '7.3 决策边界', 34: '8.1 逻辑回归的代价函数', 35: '8.2 逻辑回归的简化版代价函数', 36: '9.1 梯度下降实现', 37: '10.1 过拟合的问题', 38: '10.2 解决过拟合', 39: '10.3 正则化代价函数', 40: '10.4 正则化线性回归', 41: '10.5 正则化logistic回归', 42: '1.1 欢迎来到第二部分:高级学习算法', 43: '1.2 神经元和大脑', 44: '1.3 需求预测', 45: '1.4 例子：图像识别', 46: '2.1 神经网络中的层', 47: '2.2 更复杂的神经网络', 48: '2.3 推理：做出预测(前向传播)', 49: '3.1 代码中的推理', 50: '3.2 TensorFlow中的数据', 51: '3.3 构建一个神经网络', 52: '4.1 在一个单层中的前向传播', 53: '4.2 前向传播的一般实现', 54: '5.1 是否有路通向AGI(通用人工智能)', 55: '6.1 神经网络如何高效实现', 56: '6.2 矩阵乘法', 57: '6.3 矩阵乘法的规则', 58: '6.4 矩阵乘法代码', 59: '7.1 TensorFlow实现', 60: '7.2 训练细节', 61: '8.1 sigmoid的替代品', 62: '8.2 选择激活函数', 63: '8.3 为什么我们需要激活函数', 64: '9.1 多类', 65: '9.2 Softmax', 66: '9.3 神经网络的Softmax输出', 67: '9.4 softmax的改进实现', 68: '9.5 多个输出的分类(Optional)', 69: '10.1 高级优化方法', 70: '10.2 Additional Layer Types', 71: '11.1 决定下一步做什么', 72: '11.2 模型评估', 73: '11.3 模型选择和训练交叉验证测试集', 74: '12.1 诊断偏差和方差', 75: '12.2 正则化和偏差或方差', 76: '12.3 建立表现基准', 77: '12.4 学习曲线', 78: '12.5 再次决定下一步做什么', 79: '12.6 偏差或方差与神经网络', 80: '13.1 机器学习的迭代发展', 81: '13.2 误差分析', 82: '13.3 添加数据', 83: '13.4 迁移学习：使用其他任务中的数据', 84: '13.5 机器学习项目的完整周期', 85: '13.6 公平、偏见与伦理', 86: '14.1 Error metrics for skewed datasets', 87: '14.2 Trading off precision and recall', 88: '15.1 决策树模型', 89: '15.2 Learning Process', 90: '16.1 Measuring purity', 91: '16.2 Choosing a split_ Information Gain', 92: '16.3 Putting it together', 93: '16.4 Using one-hot encoding of categorical features', 94: '16.5 Continuous valued features', 95: '16.6 回归树 (optional)', 96: '17.1 使用多个决策树', 97: '17.2 放回抽样', 98: '17.3 随机森林算法', 99: '17.4 XGBoost', 100: '17.5 什么时候使用决策树'}\n"
     ]
    }
   ],
   "source": [
    "def get_info(url):\n",
    "    \"\"\"Get video info.\n",
    "\n",
    "    :param url: Url of the video.\n",
    "    :return: Video info, including title, date, introduction, tags and page list, wrapped into a dict.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain the html text of the url.\n",
    "    html = request(url=url).text\n",
    "\n",
    "    # Parse the html.\n",
    "    soup = BeautifulSoup(\n",
    "        markup=html,\n",
    "        features=\"html.parser\"\n",
    "    )\n",
    "\n",
    "    # Get the video title.\n",
    "    title = soup.find(\n",
    "        name=\"h1\",\n",
    "        class_=\"video-title\"\n",
    "    ).text.strip()\n",
    "\n",
    "    # Get the video date.\n",
    "    date = soup.find(\n",
    "        name=\"span\",\n",
    "        class_=\"pudate\"\n",
    "    ).text.strip()\n",
    "\n",
    "    # Get the video introduction.\n",
    "    intro = soup.find(\n",
    "        name=\"span\",\n",
    "        class_=\"desc-info-text\"\n",
    "    ).text.strip()\n",
    "\n",
    "    # Get the video tags.\n",
    "    tags = []\n",
    "    for tag_li in soup.find(\n",
    "            name=\"ul\",\n",
    "            class_=\"tag-area\"\n",
    "    ).find_all(name=\"li\"):\n",
    "        tag = tag_li.text.strip()\n",
    "        # It's really a tag if it is not an empty string.\n",
    "        if len(tag) != 0:\n",
    "            tags.append(tag)\n",
    "\n",
    "    # Get the video pages.\n",
    "    window_initial_state_script = \"\"  # The script contains page info.\n",
    "    for script in soup.find_all(name=\"script\"):\n",
    "        try:\n",
    "            if \"window.__INITIAL_STATE__\" in script.string and \"pages\" in script.string:\n",
    "                window_initial_state_script = script.string\n",
    "                break\n",
    "        except TypeError:\n",
    "            pass\n",
    "    # Extract the json from the script.\n",
    "    pattern = re.compile(pattern=r\"(\\{.*\\});\")\n",
    "    window_initial_state_json = pattern.search(string=window_initial_state_script).group(1)\n",
    "    # Convert the json to dict.\n",
    "    window_initial_state_dict = json.loads(s=window_initial_state_json)\n",
    "    # Extract page info.\n",
    "    pages = window_initial_state_dict[\"videoData\"][\"pages\"]\n",
    "    pages = {\n",
    "        int(page_dict['page']): page_dict['part']\n",
    "        for page_dict in pages\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"intro\": intro,\n",
    "        \"tags\": tags,\n",
    "        \"pages\": pages,\n",
    "    }\n",
    "\n",
    "\n",
    "bv_id = \"BV19B4y1W76i\"\n",
    "\n",
    "url = construct_url(bv_id=bv_id)\n",
    "info = get_info(url=url)\n",
    "\n",
    "for k, v in info.items():\n",
    "    print(f\"[{k}]: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bb3ab-b39b-4d38-8f9d-36bd4d0e126c",
   "metadata": {},
   "source": [
    "## 9. 分p视频内容爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d3012-9e48-4c9a-a5d7-9b78752c7dce",
   "metadata": {},
   "source": [
    "接下来，我们把每一p的视频爬下来，并以mp4形式保存。\n",
    "\n",
    "首先讲点需要用到的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1e19-f1c3-4333-9b57-3e9468ef51a5",
   "metadata": {},
   "source": [
    "### 9.1 每一P的URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122944b-5d18-4cd0-bced-874e703659e2",
   "metadata": {},
   "source": [
    "首先，点击第1p、第2p、第3p，看看url的格式。\n",
    "+ 第1p：https://www.bilibili.com/video/BV19B4y1W76i?spm_id_from=333.337.search-card.all.click&vd_source=cb6bdc56db66ac895c0f3d6912c94028\n",
    "+ 第2p：https://www.bilibili.com/video/BV19B4y1W76i?p=2&vd_source=cb6bdc56db66ac895c0f3d6912c94028\n",
    "+ 第3p：https://www.bilibili.com/video/BV19B4y1W76i?p=3&vd_source=cb6bdc56db66ac895c0f3d6912c94028\n",
    "\n",
    "够了够了。来找规律。先把没用的东西删掉。\n",
    "+ 第1p：https://www.bilibili.com/video/BV19B4y1W76i\n",
    "+ 第2p：https://www.bilibili.com/video/BV19B4y1W76i?p=2\n",
    "+ 第3p：https://www.bilibili.com/video/BV19B4y1W76i?p=3\n",
    "\n",
    "规律找到了，顺便帮第1p加点东西。\n",
    "+ 第1p：https://www.bilibili.com/video/BV19B4y1W76i?p=1\n",
    "+ 第2p：https://www.bilibili.com/video/BV19B4y1W76i?p=2\n",
    "+ 第3p：https://www.bilibili.com/video/BV19B4y1W76i?p=3\n",
    "\n",
    "然后就可以`for`遍历了~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae8e17-f36c-4b65-95dd-ea34e1f961fb",
   "metadata": {},
   "source": [
    "### 9.2 请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4b451-be60-4301-b135-ca42d42e83e8",
   "metadata": {},
   "source": [
    "使用`requests`请求的时候，有时候需要添加请求头（以字典的形式），给服务器提供更多信息。为了爬取视频内容，我们需要如下请求头。\n",
    "\n",
    "```python\n",
    "{\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',\n",
    "    'Referer': url,  # Necessary for requesting audio/video content.\n",
    "}\n",
    "```\n",
    "\n",
    "其中，`'User-Agent'`是为了告诉服务器，发出网络请求的不是一个爬虫，而是浏览器。通过`requests`发出的请求中，其请求头的`'User-Agent'`和浏览器发出的不太一样，而有些服务器会通过`'User-Agent'`判断发出请求的是不是爬虫，是就拒绝访问服务。所以把`'User-Agent`换一下，假装发出的是浏览器的请求。\n",
    "\n",
    "`'Referer'`是爬取视频内容必须的，没有就访问不了。\n",
    "\n",
    "你可能会想问怎么知道请求头需要包含这些的。说来话长，慢慢摸出来的。不过我可以告诉你在哪获取。\n",
    "\n",
    "<div align=\"center\"><img src=\"headers.png\" width=\"800\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bef441-d7e2-4195-8deb-6632764bbf94",
   "metadata": {},
   "source": [
    "现在我们将请求头的构造封装一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81979886-3584-4260-b8cf-57285695db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_headers(url):\n",
    "    \"\"\"Construct headers with the given url.\n",
    "\n",
    "    :param url: Url needed to construct the headers.\n",
    "    :return: Constructed headers.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        'Referer': url,  # Necessary for requesting audio/video content.\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36',\n",
    "    }\n",
    "\n",
    "# 使用方法\n",
    "url = construct_url(bv_id=\"BV19B4y1W76i\")\n",
    "headers = construct_headers(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ce129-f172-4ac4-9656-b122ebeba535",
   "metadata": {},
   "source": [
    "然后改改前面4.4节定义的`request`方法。除了请求头，还顺便加上了超时限制`timeout`。请求后60秒不响应就中断请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8b7ca29-2e10-4ce5-97f3-f3371639f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(url, headers, timeout=60):\n",
    "    \"\"\"Send a request.\n",
    "\n",
    "    :param url: The url to which the request is sent.\n",
    "    :param headers: Request headers.\n",
    "    :param timeout: Request timeout. Default 60s.\n",
    "    :return: Response from the server of the url.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Send the request.\n",
    "        r = requests.get(\n",
    "            url=url,\n",
    "            headers=headers,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        # Raise an exception if something goes wrong.\n",
    "        r.raise_for_status()\n",
    "\n",
    "        return r\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c449f24-19df-40ec-8eb3-8a8132f352cd",
   "metadata": {},
   "source": [
    "### 9.3 其他已知的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a737c-632e-47a2-bcee-f17bd92e419b",
   "metadata": {},
   "source": [
    "+ 由于视频为二进制数据，不能直接在html中存储，因此以**url**形式出现在html中。\n",
    "+ 直接用浏览器点视频是拿不到视频url的，需要在浏览器的网页源码（或`requests`得到的`html`）中搜索。\n",
    "+ 搜索关键字为：\"baseUrl\"。可能会搜出来很多，不知道有什么区别。应该是差不多的。用第1个就可。\n",
    "+ 注意，音频和视频的url是分开的。搜索\"baseUrl\"时，注意\"**audio**\"和\"**video**\"，两个都需要。\n",
    "+ 获取audio和video的**url**的方式和第7节 “网页内容的差异”类似。\n",
    "+ 获取audio和video的url后，再**请求其二进制内容**。通过`r.content`获取二进制内容。\n",
    "+ 得到的音频和视频的二进制数据格式都为`m4s`。可以通过二进制文件写入（`\"w\"`改成`\"wb\"`）保存。\n",
    "+ 保存后使用ffmpeg等工具将audio和video**合并**成mp4，就能得到视频内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb7e3e-0900-4a21-8307-91bd128b5650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
